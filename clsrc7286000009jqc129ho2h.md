---
title: "Note #005 : OpenAI Supergrants"
datePublished: Sun Feb 18 2024 10:00:23 GMT+0000 (Coordinated Universal Time)
cuid: clsrc7286000009jqc129ho2h
slug: note-005-openai-supergrants
tags: ai-hacker

---

J'envisage de faire une demande de subventions, ce qu'on appelle “*grants*” à OpenAI. En réalité, OpenAI a lancé en Décembre 2023 un programme de soutien à la recherche sur l'alignement dans l'IA appelé “*Superalignment Fast Grants*”. **Le délai est fixé ce jour 18 Février 2024**. Je vais donc postuler et voir ce que ça peut donner.

Être sélectionné me permettrai de m'adonner tranquillement à la recherche sans être absorbé par les tracasseries du quotidien. Le rêve est permis mais la réalité demande que je puisse monter un bon dossier. Et c'est là que les choses se compliquent. Mais je vais m'y essayer.

En réalité, ma conviction profonde est que l'originalité de l'IA vient des mécanismes d'encodage des données qu'elle propose actuellement. Et c'est cela qu'il faut étudier, à la lumière de ce qui a été fait par le passé. L'IA représente ainsi un second âge d'or de l'informatique, commencée avec la réussite de l'encodage des caractères, des images, du son et de l'audio. Le premier âge d'or de cet encodage a été porté par le CPU à travers les "*bit patterns*". Et le second âge d'or de l'encodage que porte l'IA est porté par le GPU à travers les "*matrix ou tensors patterns*".

Il faut donc revisiter, à la lumière des développements de l'informatique à travers une ressource comme "*Nand to Tetris*" de Noam Nisan et Shimon Schocken, comment différents niveaux d'abstraction ont été construits pour que l'encodage des inputs (*ASCII, Unicode, RGB, ADC/DAC*) puisse donner exactement, après traitement, l'output désiré. Entre l'input et l'output, il y a énormément d'abstractions. C'est cette réflexion à la lumière de ces différents niveaux d'abstraction qui va permettre de réduire les hallucinations produites par les modèles fondationnels d'IA.

Ma recherche se propose donc d'examiner, pour chaque abstraction (mémoire, machine virtuelle, compilateur, système d'exploitation,...etc.), son apport à l'input pour la prédiction de l'output désiré. C'est en comprenant la place de chaque abstraction qu'on pourra détecter des champs d'innovation pour les modèles d'IA afin d'avoir une prédiction comparable.

Voilà un peu comment j'envisage ma recherche, plus orientée "computer science". Bien que je n'ai pas de background comparable à tous ceux qui vont y postuler, j'espère que l'équipe m'accordera même 3 mois de tests pour leur prouver que bien que ça ne semble pas accrocheur, quelque chose d'intéressant peut en sortir.

J'ai repris les questions qu'ils ont posé pour faire une réponse en français bien sûr, avant d'aller candidater en anglais.

Voilà ci-bas une esquisse de ce sur quoi je vais m'inspirer pour remplir ma fiche de candidature.

*Short description*

*In &lt;=5 bullet points, summarize your application.*

* L'IA représente le second âge d'or de l'informatique après le premier qui a vu l'encodage réussi des inputs (texte, image, audio, vidéo) à travers les "*bit patterns*", encodage supporté par le CPU.
    
* Le second âge d'or de l'encodage que porte l'IA est porté par le GPU à travers les "*matrix ou tensors patterns*".
    
* Ma recherche partira d'une ressource comme "*Nand to Tetris*" de Noam Nisan et Shimon Schocken, pour examiner comment différents niveaux d'abstraction ont été construits pour que l'encodage des inputs (*ASCII, Unicode, RGB, ADC/DAC*) puisse donner exactement, après traitement, l'output désiré.
    
* Je vais revoir l'apport de chaque niveau d'abstraction (mémoire, machine virtuelle, système d'exploitation, compilateur,...etc) dans l'obtention de l'output désiré après traitement de l'input pour voir si cela peut être répliqué dans l'IA pour réduire les hallucinations.
    
* Volonté de démontrer le potentiel de la recherche par une période de test de trois (3) mois avec rapport.
    

*About you*

*Tell us about yourself (and any research collaborators). A paragraph or bullets will do.*

*Please describe past research and your best past work (even if an unrelated field).*

Je suis Duverger PETGA, Architecte de Solutions AWS (SAA-C03). J'ai commencé ma carrière comme magasinier avant de changer de carrière pour développeur React. Je m'intéresse depuis 2020 à l'intelligence artificielle et je me suis donné les bases en passant le "*AWS Machine Learning Foundations Nanodegree*" d'Udacity. Tout en assurant les formations sur Excel et sur la préparation aux certifications AWS, je garde un œil sur l'actualité de l'IA à travers la publication de ma lettre de l'IA ( https://lalettreia.substack.com) et je renforce mes recherches en suivant un canevas disponible à cette adresse : https://whimsical.com/curriculum-ml-ai-course-PwvhXHVjwxro3NupZa6aPc

*Links*

*Links to an online profile (website, resume, etc.) and some of your best work.*

* Mon site web personnel : https://remotepetga.netlify.app/
    
* Ma page Youtube : https://www.youtube.com/@ClubArchiCloudAWS
    
* Ma page Github : https://github.com/epegase
    
* Ma page LinkedIn : https://www.linkedin.com/in/duverger-petga-0a273096
    

*Your research project*

*In a half-page or less, describe the research you wish to pursue.*

*Please be very concrete! Include milestones and expected output.*

*You can include links, and/or upload a pdf with further details at the end of this form.*

Ma conviction profonde est que l'originalité de l'IA générative vient des mécanismes d'encodage des données qu'elle propose actuellement. Et c'est cela qu'il faut étudier, à la lumière de ce qui a été fait par le passé. L'IA représente ainsi un second âge d'or de l'informatique, commencée avec la réussite de l'encodage des caractères, des images, du son et de l'audio. Le premier âge d'or de cet encodage a été porté par le CPU à travers les "*bit patterns*". Et le second âge d'or de l'encodage que porte l'IA est porté par le GPU à travers les "*matrix ou tensors patterns*".

Il faut donc revisiter, à la lumière des développements de l'informatique à travers une ressource comme "*Nand to Tetris*" de Noam Nisan et Shimon Schocken, comment différents niveaux d'abstraction ont été construits pour que l'encodage des inputs (*ASCII, Unicode, RGB, ADC/DAC*) puisse donner exactement, après traitement, l'output désiré. Entre l'input et l'output, il y a énormément d'abstractions. C'est cette réflexion à la lumière de ces différents niveaux d'abstraction qui va permettre de réduire les hallucinations produites par les modèles fondationnels d'IA.

Ma recherche se propose donc d'examiner, pour chaque abstraction (mémoire, machine virtuelle, compilateur, système d'exploitation,...etc.), son apport à l'input pour la prédiction de l'output désiré. C'est en comprenant la place de chaque abstraction qu'on pourra détecter des champs d'innovation pour les modèles d'IA afin d'avoir une prédiction comparable.

Voilà un peu comment j'envisage ma recherche, plus orientée "computer science". Bien que je n'ai pas de background comparable à tous ceux qui vont y postuler, j'espère que l'équipe m'accordera même 3 mois de tests pour leur prouver que bien que ça ne semble pas accrocheur, quelque chose d'intéressant peut en sortir.

*Connection to alignment and safety of superhuman AI systems*

*Please briefly explain the motivation for your proposed research and how it will help with the alignment and safety of future advanced AI systems.*

Le premier mécanisme qui a été trouvé pour réduire les hallucinations a été le RAG qui s'appuie en grande partie sur les bases de données vectorielles. L'hallucination des modèles d'IA a permis l'apparition d'un nouvel élément dans l'architecture des applications d'IA... Les bases de données vectorielles. Mais c'est juste une évolution d'une abstraction (base de données) qui existait dans le premier âge d'or de l'informatique. L'alignement et la sécurité des systèmes d'IA demanderont de créer de nouvelles abstractions qui pourront s'inspirer des anciennes, mais pas juste les adapter.

*Budget*

*How much funding are you requesting?*

* 37 000 US dollars. (for 12 months)
    

*Budget description*

*Briefly break down how you intend to use the funds.*

*In addition to your mainline budget, it can be helpful for you to give a lower and an upper bound (what are smaller/larger versions of the project we could fund?)*.

* Salary (+ Health Insurance) : 2500 US x 12 = 30 000 US
    
* Books and others subscriptions : 2500 US
    
* GPU Cloud (1000 hours, ex. Lambda Labs) : 2500 US
    
* Others furnitures : 2000 US